"""
V2 Content Selection Models for LangFlix.

These models are used for the V2 workflow where translations are provided
from Netflix subtitles rather than generated by the LLM.
"""
from typing import List, Optional
from pydantic import BaseModel, Field


class V2VocabularyAnnotation(BaseModel):
    """
    Vocabulary word annotation for dynamic overlays.
    Points to source dialogue by index.
    """
    word: str = Field(description="The vocabulary word or phrase to annotate")
    translation: str = Field(
        description="Translation of the word in target language (required from LLM)"
    )
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues where this word appears",
        ge=0
    )


class V2Narration(BaseModel):
    """
    Narration commentary for dynamic overlays.
    Timestamped commentary in target language displayed separately from subtitles.
    """
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues when this narration should appear",
        ge=0
    )
    text: str = Field(description="Narration text in target language")
    type: str = Field(
        default="commentary",
        description="Narration type: hook, highlight, reaction, tension, commentary"
    )


class V2ExpressionAnnotation(BaseModel):
    """
    Expression/idiom annotation for dynamic overlays.
    Multi-word phrases (distinct from single-word V2VocabularyAnnotation).
    """
    expression: str = Field(description="The idiom or multi-word phrase to annotate")
    translation: str = Field(
        description="Translation of the expression in target language (required from LLM)"
    )
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues where this expression appears",
        ge=0
    )


class V2ContentSelection(BaseModel):
    """
    V2 model for content selection result.
    
    Key difference from V1: Uses indices to reference dialogues instead of
    including translated text directly. Translations come from subtitle files.
    """
    # Generated content (in target language)
    title: Optional[str] = Field(
        default=None,
        description="Catchy title in target language (8-15 words)"
    )
    viral_title: Optional[str] = Field(
        default=None,
        description="Iconic, meme-worthy quote from the scene in source language (3-12 words)"
    )
    catchy_keywords: Optional[List[str]] = Field(
        default=None,
        description="2-3 engaging phrases in target language"
    )
    intro_hook: Optional[str] = Field(
        default=None,
        description="Short hook question in target language"
    )
    
    # Expression identification (from source dialogues)
    expression: str = Field(
        description="The key expression/phrase to teach from source language"
    )
    expression_dialogue_index: int = Field(
        description="0-indexed position in source_dialogues containing the expression",
        ge=0
    )
    
    # Context boundaries (indices into dialogues arrays)
    context_start_index: int = Field(
        description="First dialogue index to include in video clip",
        ge=0
    )
    context_end_index: int = Field(
        description="Last dialogue index to include (inclusive)",
        ge=0
    )
    
    # Additional annotations
    vocabulary_annotations: Optional[List[V2VocabularyAnnotation]] = Field(
        default=None,
        description="2-5 vocabulary words for dynamic overlays"
    )
    narrations: Optional[List[V2Narration]] = Field(
        default=None,
        description="3-6 narration commentaries in target language"
    )
    expression_annotations: Optional[List[V2ExpressionAnnotation]] = Field(
        default=None,
        description="1-3 idiom/phrase annotations for dynamic overlays"
    )
    similar_expressions: List[str] = Field(
        default_factory=list,
        description="2-3 alternative expressions in source language"
    )
    scene_type: Optional[str] = Field(
        default=None,
        description="Type of scene: humor, drama, tension, emotional, witty, sexy, surprising"
    )
    
    # Computed fields (populated after matching with subtitles)
    expression_translation: Optional[str] = Field(
        default=None,
        description="Translation of expression (populated from target subtitles)"
    )
    context_start_time: Optional[str] = Field(
        default=None,
        description="Start timestamp (populated from source subtitles)"
    )
    context_end_time: Optional[str] = Field(
        default=None,
        description="End timestamp (populated from source subtitles)"
    )


class V2ContentSelectionResponse(BaseModel):
    """
    Complete response from V2 content selection LLM call.
    """
    expressions: List[V2ContentSelection] = Field(
        default_factory=list,
        description="List of selected expressions"
    )


def enrich_content_selection(
    selection: V2ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> V2ContentSelection:
    """
    Enrich a V2ContentSelection with data from subtitle files.
    
    Populates:
    - expression_translation: From target dialogue at expression_dialogue_index
    - context_start_time: From source dialogue at context_start_index
    - context_end_time: From source dialogue at context_end_index
    
    Args:
        selection: V2ContentSelection to enrich
        source_dialogues: List of source dialogue dicts with 'text', 'start', 'end'
        target_dialogues: List of target dialogue dicts with 'text', 'start', 'end'
        
    Returns:
        Enriched V2ContentSelection
    """
    # Get expression translation from target dialogues
    if 0 <= selection.expression_dialogue_index < len(target_dialogues):
        target_line = target_dialogues[selection.expression_dialogue_index]
        selection.expression_translation = target_line.get('text', '')
    
    # Get context start time from source dialogues
    if 0 <= selection.context_start_index < len(source_dialogues):
        start_line = source_dialogues[selection.context_start_index]
        selection.context_start_time = start_line.get('start', '')
    
    # Get context end time from source dialogues
    if 0 <= selection.context_end_index < len(source_dialogues):
        end_line = source_dialogues[selection.context_end_index]
        selection.context_end_time = end_line.get('end', '')
    
    return selection


def convert_v2_to_v1_format(
    selection: V2ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> dict:
    """
    Convert V2ContentSelection to V1 ExpressionAnalysis-compatible dict.
    
    This allows V2 content selections to work with existing video generation
    pipeline that expects V1 format.
    
    Args:
        selection: V2ContentSelection (should be enriched first)
        source_dialogues: Source dialogue dicts
        target_dialogues: Target dialogue dicts
        
    Returns:
        Dict compatible with V1 ExpressionAnalysis
    """
    # Extract dialogue text and translations for context range
    start_idx = selection.context_start_index
    end_idx = selection.context_end_index + 1  # Make inclusive
    
    dialogues = [d.get('text', '') for d in source_dialogues[start_idx:end_idx]]
    translations = [d.get('text', '') for d in target_dialogues[start_idx:end_idx]]
    
    # Get expression dialogue
    expr_idx = selection.expression_dialogue_index
    expression_dialogue = source_dialogues[expr_idx].get('text', '') if expr_idx < len(source_dialogues) else ''
    expression_dialogue_translation = target_dialogues[expr_idx].get('text', '') if expr_idx < len(target_dialogues) else ''
    
    # Build vocabulary annotations with translations from LLM
    vocab_annotations = []
    if selection.vocabulary_annotations:
        for va in selection.vocabulary_annotations:
            vocab_annotations.append({
                'word': va.word,
                'translation': va.translation,  # Use LLM-provided translation, not subtitle
                'dialogue_index': max(0, va.dialogue_index - start_idx)  # Fix Timestamp Drift: Normalize to context
            })
    
    # Build narrations (normalize dialogue_index to context range)
    narrations = []
    if selection.narrations:
        for narr in selection.narrations:
            narrations.append({
                'dialogue_index': max(0, narr.dialogue_index - start_idx),
                'text': narr.text,
                'type': narr.type
            })
    
    # Build expression annotations with translations from LLM
    expr_annotations = []
    if selection.expression_annotations:
        for ea in selection.expression_annotations:
            expr_annotations.append({
                'expression': ea.expression,
                'translation': ea.translation,  # Use LLM-provided translation, not subtitle
                'dialogue_index': max(0, ea.dialogue_index - start_idx)
            })
    
    # Calculate expression timestamps (fallback to full dialogue line duration)
    # V2 doesn't provide word-level timing, so we use the containing dialogue line
    expression_start_time = ''
    expression_end_time = ''
    if 0 <= selection.expression_dialogue_index < len(source_dialogues):
        expr_dialogue_line = source_dialogues[selection.expression_dialogue_index]
        expression_start_time = expr_dialogue_line.get('start', '')
        expression_end_time = expr_dialogue_line.get('end', '')

    return {
        'title': selection.title,
        'viral_title': selection.viral_title,
        'dialogues': dialogues,
        'translation': translations,
        'expression_dialogue': expression_dialogue,
        'expression_dialogue_translation': expression_dialogue_translation,
        'expression': selection.expression,
        'expression_translation': selection.expression_translation or '',
        'intro_hook': selection.intro_hook,
        'context_start_time': selection.context_start_time,
        'context_end_time': selection.context_end_time,
        'expression_start_time': expression_start_time,
        'expression_end_time': expression_end_time,
        'similar_expressions': selection.similar_expressions,
        'scene_type': selection.scene_type,
        'catchy_keywords': selection.catchy_keywords,
        'vocabulary_annotations': vocab_annotations,
        'narrations': narrations,
        'expression_annotations': expr_annotations,
    }

