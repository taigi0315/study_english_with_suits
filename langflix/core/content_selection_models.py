"""
V2 Content Selection Models for LangFlix.

These models are used for the V2 workflow where translations are provided
from Netflix subtitles rather than generated by the LLM.
"""
from typing import List, Optional
from pydantic import BaseModel, Field


class V2VocabularyAnnotation(BaseModel):
    """
    Vocabulary word annotation for dynamic overlays.
    Points to source dialogue by index.
    """
    word: str = Field(description="The vocabulary word or phrase to annotate")
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues where this word appears",
        ge=0
    )


class V2ContentSelection(BaseModel):
    """
    V2 model for content selection result.
    
    Key difference from V1: Uses indices to reference dialogues instead of
    including translated text directly. Translations come from subtitle files.
    """
    # Generated content (in target language)
    title: Optional[str] = Field(
        default=None,
        description="Catchy title in target language (8-15 words)"
    )
    catchy_keywords: Optional[List[str]] = Field(
        default=None,
        description="2-3 engaging phrases in target language"
    )
    intro_hook: Optional[str] = Field(
        default=None,
        description="Short hook question in target language"
    )
    
    # Expression identification (from source dialogues)
    expression: str = Field(
        description="The key expression/phrase to teach from source language"
    )
    expression_dialogue_index: int = Field(
        description="0-indexed position in source_dialogues containing the expression",
        ge=0
    )
    
    # Context boundaries (indices into dialogues arrays)
    context_start_index: int = Field(
        description="First dialogue index to include in video clip",
        ge=0
    )
    context_end_index: int = Field(
        description="Last dialogue index to include (inclusive)",
        ge=0
    )
    
    # Additional annotations
    vocabulary_annotations: Optional[List[V2VocabularyAnnotation]] = Field(
        default=None,
        description="2-5 vocabulary words for dynamic overlays"
    )
    similar_expressions: List[str] = Field(
        default_factory=list,
        description="2-3 alternative expressions in source language"
    )
    scene_type: Optional[str] = Field(
        default=None,
        description="Type of scene: humor, drama, tension, emotional, witty, sexy, surprising"
    )
    
    # Computed fields (populated after matching with subtitles)
    expression_translation: Optional[str] = Field(
        default=None,
        description="Translation of expression (populated from target subtitles)"
    )
    context_start_time: Optional[str] = Field(
        default=None,
        description="Start timestamp (populated from source subtitles)"
    )
    context_end_time: Optional[str] = Field(
        default=None,
        description="End timestamp (populated from source subtitles)"
    )


class V2ContentSelectionResponse(BaseModel):
    """
    Complete response from V2 content selection LLM call.
    """
    expressions: List[V2ContentSelection] = Field(
        default_factory=list,
        description="List of selected expressions"
    )


def enrich_content_selection(
    selection: V2ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> V2ContentSelection:
    """
    Enrich a V2ContentSelection with data from subtitle files.
    
    Populates:
    - expression_translation: From target dialogue at expression_dialogue_index
    - context_start_time: From source dialogue at context_start_index
    - context_end_time: From source dialogue at context_end_index
    
    Args:
        selection: V2ContentSelection to enrich
        source_dialogues: List of source dialogue dicts with 'text', 'start', 'end'
        target_dialogues: List of target dialogue dicts with 'text', 'start', 'end'
        
    Returns:
        Enriched V2ContentSelection
    """
    # Get expression translation from target dialogues
    if 0 <= selection.expression_dialogue_index < len(target_dialogues):
        target_line = target_dialogues[selection.expression_dialogue_index]
        selection.expression_translation = target_line.get('text', '')
    
    # Get context start time from source dialogues
    if 0 <= selection.context_start_index < len(source_dialogues):
        start_line = source_dialogues[selection.context_start_index]
        selection.context_start_time = start_line.get('start', '')
    
    # Get context end time from source dialogues
    if 0 <= selection.context_end_index < len(source_dialogues):
        end_line = source_dialogues[selection.context_end_index]
        selection.context_end_time = end_line.get('end', '')
    
    return selection


def convert_v2_to_v1_format(
    selection: V2ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> dict:
    """
    Convert V2ContentSelection to V1 ExpressionAnalysis-compatible dict.
    
    This allows V2 content selections to work with existing video generation
    pipeline that expects V1 format.
    
    Args:
        selection: V2ContentSelection (should be enriched first)
        source_dialogues: Source dialogue dicts
        target_dialogues: Target dialogue dicts
        
    Returns:
        Dict compatible with V1 ExpressionAnalysis
    """
    # Extract dialogue text and translations for context range
    start_idx = selection.context_start_index
    end_idx = selection.context_end_index + 1  # Make inclusive
    
    dialogues = [d.get('text', '') for d in source_dialogues[start_idx:end_idx]]
    translations = [d.get('text', '') for d in target_dialogues[start_idx:end_idx]]
    
    # Get expression dialogue
    expr_idx = selection.expression_dialogue_index
    expression_dialogue = source_dialogues[expr_idx].get('text', '') if expr_idx < len(source_dialogues) else ''
    expression_dialogue_translation = target_dialogues[expr_idx].get('text', '') if expr_idx < len(target_dialogues) else ''
    
    # Build vocabulary annotations with translations
    vocab_annotations = []
    if selection.vocabulary_annotations:
        for va in selection.vocabulary_annotations:
            if 0 <= va.dialogue_index < len(target_dialogues):
                vocab_annotations.append({
                    'word': va.word,
                    'translation': target_dialogues[va.dialogue_index].get('text', '')[:50],  # Truncate
                    'dialogue_index': va.dialogue_index
                })
    
    return {
        'title': selection.title,
        'dialogues': dialogues,
        'translation': translations,
        'expression_dialogue': expression_dialogue,
        'expression_dialogue_translation': expression_dialogue_translation,
        'expression': selection.expression,
        'expression_translation': selection.expression_translation or '',
        'intro_hook': selection.intro_hook,
        'context_start_time': selection.context_start_time,
        'context_end_time': selection.context_end_time,
        'similar_expressions': selection.similar_expressions,
        'scene_type': selection.scene_type,
        'catchy_keywords': selection.catchy_keywords,
        'vocabulary_annotations': vocab_annotations,
    }
