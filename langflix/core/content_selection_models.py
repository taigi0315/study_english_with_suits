"""
Content Selection Models for LangFlix.

These models are used for dual-subtitle workflow where translations are provided
from Netflix subtitles rather than generated by the LLM.
"""
from typing import List, Optional
from pydantic import BaseModel, Field


class VocabularyAnnotation(BaseModel):
    """
    Vocabulary word annotation for dynamic overlays.
    Points to source dialogue by index.
    """
    word: str = Field(description="The vocabulary word or phrase to annotate")
    translation: str = Field(
        description="Translation of the word in target language (required from LLM)"
    )
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues where this word appears",
        ge=0
    )


class Narration(BaseModel):
    """
    Narration commentary for dynamic overlays.
    Timestamped commentary in target language displayed separately from subtitles.
    """
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues when this narration should appear",
        ge=0
    )
    text: str = Field(description="Narration text in target language")
    type: str = Field(
        default="commentary",
        description="Narration type: hook, highlight, reaction, tension, commentary"
    )


class ExpressionAnnotation(BaseModel):
    """
    Expression/idiom annotation for dynamic overlays.
    Multi-word phrases (distinct from single-word VocabularyAnnotation).
    """
    expression: str = Field(description="The idiom or multi-word phrase to annotate")
    translation: str = Field(
        description="Translation of the expression in target language (required from LLM)"
    )
    dialogue_index: int = Field(
        default=0,
        description="0-indexed position in source_dialogues where this expression appears",
        ge=0
    )


class ContentSelection(BaseModel):
    """
    Model for content selection result.

    Uses indices to reference dialogues. Translations come from subtitle files.

    REQUIRED fields that must come from LLM (no fallbacks):
    - title: source language
    - title_translation: target language
    - expression: source language
    - expression_translation: target language
    """
    # REQUIRED content - fail if LLM doesn't provide
    title: str = Field(
        description="Catchy title in SOURCE language (meme-worthy quote)"
    )
    title_translation: str = Field(
        description="Title translated to TARGET language (for video overlay)"
    )
    viral_title: Optional[str] = Field(
        default=None,
        description="Iconic, meme-worthy quote from the scene in source language (3-12 words)"
    )
    catchy_keywords: Optional[List[str]] = Field(
        default=None,
        description="2-3 engaging phrases in target language"
    )
    intro_hook: Optional[str] = Field(
        default=None,
        description="Short hook question in target language"
    )
    
    # Expression identification (from source dialogues)
    expression: str = Field(
        description="The key expression/phrase to teach from source language"
    )
    expression_dialogue_index: int = Field(
        description="0-indexed position in source_dialogues containing the expression",
        ge=0
    )
    
    # Context boundaries (indices into dialogues arrays)
    context_start_index: int = Field(
        description="First dialogue index to include in video clip",
        ge=0
    )
    context_end_index: int = Field(
        description="Last dialogue index to include (inclusive)",
        ge=0
    )
    
    # Additional annotations
    vocabulary_annotations: Optional[List[VocabularyAnnotation]] = Field(
        default=None,
        description="2-5 vocabulary words for dynamic overlays"
    )
    narrations: Optional[List[Narration]] = Field(
        default=None,
        description="3-6 narration commentaries in target language"
    )
    expression_annotations: Optional[List[ExpressionAnnotation]] = Field(
        default=None,
        description="1-3 idiom/phrase annotations for dynamic overlays"
    )
    similar_expressions: List[str] = Field(
        default_factory=list,
        description="2-3 alternative expressions in source language"
    )
    scene_type: Optional[str] = Field(
        default=None,
        description="Type of scene: humor, drama, tension, emotional, witty, sexy, surprising"
    )
    
    # REQUIRED from LLM - do NOT fallback to subtitle text
    expression_translation: str = Field(
        description="LITERAL translation of expression in target language (required from LLM)"
    )
    context_start_time: Optional[str] = Field(
        default=None,
        description="Start timestamp (populated from source subtitles)"
    )
    context_end_time: Optional[str] = Field(
        default=None,
        description="End timestamp (populated from source subtitles)"
    )


class ContentSelectionResponse(BaseModel):
    """
    Complete response from content selection LLM call.
    """
    expressions: List[ContentSelection] = Field(
        default_factory=list,
        description="List of selected expressions"
    )


def enrich_from_subtitles(
    selection: ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> ContentSelection:
    """
    Enrich a ContentSelection with timestamp data from subtitle files.
    
    NOTE: expression_translation is REQUIRED from LLM and already validated.
    This function only populates timestamps (context_start_time, context_end_time).
    
    Args:
        selection: ContentSelection to enrich
        source_dialogues: List of source dialogue dicts with 'text', 'start', 'end'
        target_dialogues: List of target dialogue dicts with 'text', 'start', 'end'
        
    Returns:
        Enriched ContentSelection
    """
    # Get context start time from source dialogues
    if 0 <= selection.context_start_index < len(source_dialogues):
        start_line = source_dialogues[selection.context_start_index]
        selection.context_start_time = start_line.get('start', '')
    
    # Get context end time from source dialogues
    if 0 <= selection.context_end_index < len(source_dialogues):
        end_line = source_dialogues[selection.context_end_index]
        selection.context_end_time = end_line.get('end', '')
    
    return selection


def convert_to_legacy_format(
    selection: ContentSelection,
    source_dialogues: List[dict],
    target_dialogues: List[dict],
) -> dict:
    """
    Convert ContentSelection to legacy ExpressionAnalysis-compatible dict.

    This allows content selections to work with existing video generation pipeline.

    Args:
        selection: ContentSelection (should be enriched first)
        source_dialogues: Source dialogue dicts
        target_dialogues: Target dialogue dicts

    Returns:
        Dict compatible with legacy ExpressionAnalysis format
    """
    # Extract dialogue text and translations for context range
    start_idx = selection.context_start_index
    end_idx = selection.context_end_index + 1  # Make inclusive
    
    # Extract dialogue data - include timing for subtitle generation
    context_source_dialogues = source_dialogues[start_idx:end_idx]
    context_target_dialogues = target_dialogues[start_idx:end_idx]
    
    dialogues = [d.get('text', '') for d in context_source_dialogues]
    translations = [d.get('text', '') for d in context_target_dialogues]
    
    # Also include full dialogue objects with timing for subtitle generation
    dialogue_entries = []
    for i, (src, tgt) in enumerate(zip(context_source_dialogues, context_target_dialogues)):
        dialogue_entries.append({
            'text': src.get('text', ''),
            'translation': tgt.get('text', ''),
            'start_time': src.get('start', ''),
            'end_time': src.get('end', ''),
        })
    
    # Get expression dialogue
    expr_idx = selection.expression_dialogue_index
    expression_dialogue = source_dialogues[expr_idx].get('text', '') if expr_idx < len(source_dialogues) else ''
    expression_dialogue_translation = target_dialogues[expr_idx].get('text', '') if expr_idx < len(target_dialogues) else ''
    
    # Build vocabulary annotations with translations from LLM
    vocab_annotations = []
    if selection.vocabulary_annotations:
        for va in selection.vocabulary_annotations:
            vocab_annotations.append({
                'word': va.word,
                'translation': va.translation,  # Use LLM-provided translation, not subtitle
                'dialogue_index': max(0, va.dialogue_index - start_idx)  # Fix Timestamp Drift: Normalize to context
            })
    
    # Build narrations (normalize dialogue_index to context range)
    narrations = []
    if selection.narrations:
        for narr in selection.narrations:
            narrations.append({
                'dialogue_index': max(0, narr.dialogue_index - start_idx),
                'text': narr.text,
                'type': narr.type
            })
    
    # Build expression annotations with translations from LLM
    expr_annotations = []
    if selection.expression_annotations:
        for ea in selection.expression_annotations:
            expr_annotations.append({
                'expression': ea.expression,
                'translation': ea.translation,  # Use LLM-provided translation, not subtitle
                'dialogue_index': max(0, ea.dialogue_index - start_idx)
            })
    
    # Calculate expression timestamps (fallback to full dialogue line duration)
    # Dual-subtitle mode doesn't provide word-level timing, so we use the containing dialogue line
    expression_start_time = ''
    expression_end_time = ''
    if 0 <= selection.expression_dialogue_index < len(source_dialogues):
        expr_dialogue_line = source_dialogues[selection.expression_dialogue_index]
        expression_start_time = expr_dialogue_line.get('start', '')
        expression_end_time = expr_dialogue_line.get('end', '')

    return {
        'title': selection.title,
        'title_translation': selection.title_translation,
        'viral_title': selection.viral_title,
        'dialogues': dialogues,
        'translation': translations,
        'dialogue_entries': dialogue_entries,  # Full dialogue objects with timing for subtitle generation
        'expression_dialogue': expression_dialogue,
        'expression_dialogue_translation': expression_dialogue_translation,
        'expression': selection.expression,
        'expression_translation': selection.expression_translation or '',
        'intro_hook': selection.intro_hook,
        'context_start_time': selection.context_start_time,
        'context_end_time': selection.context_end_time,
        'expression_start_time': expression_start_time,
        'expression_end_time': expression_end_time,
        'similar_expressions': selection.similar_expressions,
        'scene_type': selection.scene_type,
        'catchy_keywords': selection.catchy_keywords,
        'vocabulary_annotations': vocab_annotations,
        'narrations': narrations,
        'expression_annotations': expr_annotations,
    }

