# [TICKET-007] Implement Parallel LLM Request Processing for Faster Expression Analysis

## Priority
- [ ] Critical (System stability, security, data loss risk)
- [x] High (Performance issues, significant tech debt)
- [ ] Medium (Code quality, maintainability improvements)
- [ ] Low (Nice-to-have refactorings)

## Type
- [ ] Refactoring
- [x] Performance Optimization
- [ ] Test Coverage
- [ ] Bug Fix
- [ ] Security Issue
- [ ] Technical Debt
- [ ] Code Duplication

## Impact Assessment
**Business Impact:**
- í˜„ìž¬ ìˆœì°¨ ì²˜ë¦¬ë¡œ expression ë¶„ì„ì´ ë§¤ìš° ëŠë¦¼ (ê° ì²­í¬ë§ˆë‹¤ LLM í˜¸ì¶œ ëŒ€ê¸°)
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ì„ 3-5ë°° ê°œì„  ê°€ëŠ¥
- ë” ë‚˜ì€ ì‚¬ìš©ìž ê²½í—˜ê³¼ ë” ë¹ ë¥¸ ê²°ê³¼ ì œê³µ

**Technical Impact:**
- ì˜í–¥ë°›ëŠ” ëª¨ë“ˆ: `langflix/main.py`, `langflix/core/parallel_processor.py`, `langflix/core/expression_analyzer.py`
- ì˜ˆìƒ ë³€ê²½ íŒŒì¼: 5-7ê°œ
- ì´ë¯¸ `ParallelProcessor`ê°€ êµ¬í˜„ë˜ì–´ ìžˆìœ¼ë‚˜ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

**Effort Estimate:**
- Medium (1-3 days)

## Problem Description

### Current State
**Location:** `langflix/main.py:391-457`

í˜„ìž¬ expression ë¶„ì„ì´ ì™„ì „ížˆ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤:

```python
def _analyze_expressions(self, max_expressions: int = None, ...):
    """Analyze expressions from subtitle chunks"""
    all_expressions = []
    
    # In test mode, process only the first chunk
    chunks_to_process = [self.chunks[0]] if test_mode and self.chunks else self.chunks
    
    for i, chunk in enumerate(chunks_to_process):  # âŒ ìˆœì°¨ ë°˜ë³µ
        if max_expressions is not None and len(all_expressions) >= max_expressions:
            break
            
        try:
            # Each call waits for LLM response - slow!
            expressions = analyze_chunk(chunk, language_level, self.language_code, save_llm_output, output_dir)
            if expressions:
                all_expressions.extend(expressions)
```

**ë¬¸ì œì :**
- ê° ì²­í¬ë§ˆë‹¤ LLM API í˜¸ì¶œì„ ê¸°ë‹¤ë¦¼
- 10ê°œ ì²­í¬ê°€ ìžˆë‹¤ë©´ ìˆœì°¨ ì²˜ë¦¬ ì‹œê°„ = ê° ì²­í¬ë³„ 5ì´ˆ Ã— 10 = 50ì´ˆ
- ë³‘ë ¬ ì²˜ë¦¬ ì‹œ 3-5ì´ˆë¡œ ë‹¨ì¶• ê°€ëŠ¥

### Root Cause Analysis
- `ParallelProcessor`ëŠ” ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìžˆìŒ (`langflix/core/parallel_processor.py`)
- `ExpressionBatchProcessor`ë„ ì¡´ìž¬í•˜ì§€ë§Œ ì‹¤ì œ `LangFlixPipeline`ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
- ê¸°ì¡´ ì½”ë“œê°€ ë‹¨ìˆœí•œ ìˆœì°¨ ì²˜ë¦¬ íŒ¨í„´ìœ¼ë¡œ ìž‘ì„±ë˜ì—ˆê³  ì´í›„ ë³‘ë ¬ ì²˜ë¦¬ê°€ í†µí•©ë˜ì§€ ì•ŠìŒ

### Evidence
- `langflix/core/parallel_processor.py:168-229`: `ExpressionBatchProcessor` êµ¬í˜„ë˜ì–´ ìžˆìŒ
- `langflix/core/parallel_processor.py:352-372`: `process_expressions_parallel` í•¨ìˆ˜ ì¡´ìž¬
- `langflix/main.py:391-457`: ì‹¤ì œë¡œëŠ” ìˆœì°¨ ë°˜ë³µë¬¸ ì‚¬ìš©
- grep ê²°ê³¼: `ExpressionBatchProcessor`ê°€ ì½”ë“œë² ì´ìŠ¤ ì–´ë””ì—ì„œë„ ì‹¤ì œë¡œ import/ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

## Proposed Solution

### Approach
1. `LangFlixPipeline`ì˜ `_analyze_expressions`ë¥¼ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë³€ê²½
2. ê¸°ì¡´ `ExpressionBatchProcessor` í™œìš©
3. ì§„í–‰ ìƒí™© ì½œë°± ìœ ì§€
4. ì„¤ì •ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”/ë¹„í™œì„±í™” ê°€ëŠ¥í•˜ê²Œ

### Implementation Details

#### Step 1: ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì„¤ì •
`langflix/config/default.yaml`ì— ì¶”ê°€:

```yaml
# Expression processing configuration
expression:
  llm:
    # Parallel processing configuration
    parallel_processing:
      enabled: true
      max_workers: null  # null = auto-detect based on CPU count
      timeout_per_chunk: 300  # seconds
      batch_size: null  # null = process all chunks at once
    
    # Chunking configuration
    min_expressions_per_chunk: 1
    max_expressions_per_chunk: 3
    max_llm_input_length: 5000
```

#### Step 2: LangFlixPipeline ë¦¬íŒ©í† ë§
`langflix/main.py`ì˜ `_analyze_expressions` ë©”ì„œë“œ ë³€ê²½:

```python
def _analyze_expressions(self, max_expressions: int = None, language_level: str = None, save_llm_output: bool = False, test_mode: bool = False) -> List[ExpressionAnalysis]:
    """Analyze expressions from subtitle chunks - PARALLEL VERSION"""
    
    # In test mode, process only the first chunk
    chunks_to_process = [self.chunks[0]] if test_mode and self.chunks else self.chunks
    
    # Check if parallel processing is enabled
    from langflix import settings
    parallel_enabled = settings.get_parallel_llm_processing_enabled()
    
    if parallel_enabled and len(chunks_to_process) > 1:
        logger.info(f"Using PARALLEL processing for {len(chunks_to_process)} chunks")
        return self._analyze_expressions_parallel(chunks_to_process, max_expressions, language_level, save_llm_output)
    else:
        logger.info(f"Using SEQUENTIAL processing for {len(chunks_to_process)} chunks")
        return self._analyze_expressions_sequential(chunks_to_process, max_expressions, language_level, save_llm_output, test_mode)

def _analyze_expressions_parallel(
    self, 
    chunks: List[List[Dict[str, Any]]], 
    max_expressions: int = None, 
    language_level: str = None,
    save_llm_output: bool = False
) -> List[ExpressionAnalysis]:
    """Analyze expressions in parallel using ExpressionBatchProcessor"""
    from langflix.core.parallel_processor import get_expression_processor
    from langflix.core.expression_analyzer import analyze_chunk
    from langflix import settings
    from pathlib import Path
    
    # Get parallel processor configuration
    max_workers = settings.get_parallel_llm_max_workers()
    timeout_per_chunk = settings.get_parallel_llm_timeout()
    
    # Get output directory for LLM outputs
    output_dir = None
    if save_llm_output:
        try:
            metadata_paths = self.paths.get('episode', {}).get('metadata', {})
            if metadata_paths and 'llm_outputs' in metadata_paths:
                output_dir = str(metadata_paths['llm_outputs'])
            else:
                episode_dir = self.paths.get('episode', {}).get('episode_dir')
                if episode_dir:
                    output_dir = str(episode_dir / 'llm_outputs')
                    Path(output_dir).mkdir(parents=True, exist_ok=True)
        except (KeyError, AttributeError) as e:
            logger.warning(f"Could not determine LLM output directory: {e}")
            output_dir = None
    
    # Progress callback
    completed_chunks = [0]
    total_chunks = len(chunks)
    
    def progress_callback(completed: int, total: int):
        completed_chunks[0] = completed
        progress_pct = int((completed / total) * 100) if total > 0 else 0
        logger.info(f"Analyzed {completed}/{total} chunks ({progress_pct}%)")
        if self.progress_callback:
            self.progress_callback(30 + (progress_pct * 0.5), f"Analyzing expressions... {completed}/{total} chunks")
    
    # Create ExpressionBatchProcessor with configuration
    processor = ExpressionBatchProcessor(max_workers=max_workers)
    
    # Analyze chunks in parallel
    logger.info(f"Starting parallel analysis of {len(chunks)} chunks with {max_workers} workers")
    start_time = time.time()
    
    # Note: ExpressionBatchProcessor uses analyze_chunk internally
    # We need to pass save_output parameter through
    # Since processor doesn't support this, we'll create a wrapper
    results = []
    for i, chunk in enumerate(chunks):
        if max_expressions is not None and len(results) >= max_expressions:
            break
        
        try:
            expressions = analyze_chunk(
                chunk, 
                language_level, 
                self.language_code, 
                save_output=save_llm_output,
                output_dir=output_dir
            )
            results.extend(expressions)
        except Exception as e:
            logger.error(f"Error analyzing chunk {i+1}: {e}")
            continue
    
    duration = time.time() - start_time
    logger.info(f"Parallel analysis complete: {len(results)} expressions in {duration:.2f}s")
    
    # Limit to max_expressions
    limited_expressions = results[:max_expressions] if max_expressions else results
    logger.info(f"Total expressions to process: {len(limited_expressions)}")
    
    # Find expression timing for each expression
    logger.info("Finding exact expression timings from subtitles...")
    for expression in limited_expressions:
        try:
            expression_start, expression_end = self.subtitle_processor.find_expression_timing(expression)
            expression.expression_start_time = expression_start
            expression.expression_end_time = expression_end
            logger.info(f"Expression '{expression.expression}' timing: {expression_start} to {expression_end}")
        except Exception as e:
            logger.warning(f"Could not find timing for expression '{expression.expression}': {e}")
    
    return limited_expressions

def _analyze_expressions_sequential(self, ...):
    """Original sequential implementation (kept for fallback)"""
    # Keep existing implementation as-is
    ...
```

ì‹¤ì œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ `ExpressionBatchProcessor`ê°€ `save_output` íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ë„ë¡ ìˆ˜ì • í•„ìš”:

#### Step 3: ExpressionBatchProcessor ì—…ë°ì´íŠ¸
`langflix/core/parallel_processor.py` ìˆ˜ì •:

```python
def analyze_expression_chunks(
    self,
    chunks: List[List[Dict[str, Any]]],
    language_level: str = None,
    language_code: str = "ko",
    save_output: bool = False,
    output_dir: str = None,
    progress_callback: Optional[Callable[[int, int], None]] = None
) -> List[List[Any]]:
    """
    Analyze multiple expression chunks in parallel
    
    Args:
        chunks: List of subtitle chunks
        language_level: Target language level
        language_code: Target language code
        save_output: Whether to save LLM outputs
        output_dir: Directory to save LLM outputs
        progress_callback: Progress callback
        
    Returns:
        List of expression analysis results
    """
    from langflix.core.expression_analyzer import analyze_chunk
    
    # Create tasks for each chunk
    tasks = []
    for i, chunk in enumerate(chunks):
        task = ProcessingTask(
            task_id=f"chunk_{i}",
            function=analyze_chunk,
            args=(chunk, language_level, language_code),
            kwargs={"save_output": save_output, "output_dir": output_dir},
            priority=len(chunk)  # Prioritize larger chunks
        )
        tasks.append(task)
    
    # Process in parallel
    results = self.processor.process_batch(tasks, progress_callback)
    
    # Extract successful results
    successful_results = []
    for result in results:
        if result.success:
            successful_results.append(result.result)
        else:
            logger.error(f"Chunk analysis failed: {result.error}")
            successful_results.append([])  # Empty list for failed chunks
    
    return successful_results
```

#### Step 4: Settings ì ‘ê·¼ìž ì¶”ê°€
`langflix/settings.py`ì— ì¶”ê°€:

```python
def get_parallel_llm_processing_enabled() -> bool:
    """Check if parallel LLM processing is enabled"""
    expression_config = get_expression_config()
    parallel_config = expression_config.get('llm', {}).get('parallel_processing', {})
    return parallel_config.get('enabled', True)

def get_parallel_llm_max_workers() -> Optional[int]:
    """Get max workers for parallel LLM processing"""
    expression_config = get_expression_config()
    parallel_config = expression_config.get('llm', {}).get('parallel_processing', {})
    max_workers = parallel_config.get('max_workers')
    if max_workers is None:
        # Auto-detect based on CPU count
        import multiprocessing
        return multiprocessing.cpu_count()
    return max_workers

def get_parallel_llm_timeout() -> float:
    """Get timeout per chunk for parallel processing"""
    expression_config = get_expression_config()
    parallel_config = expression_config.get('llm', {}).get('parallel_processing', {})
    return parallel_config.get('timeout_per_chunk', 300)
```

#### Step 5: API Routesì—ë„ ë³‘ë ¬ ì²˜ë¦¬ ì ìš©
`langflix/api/routes/jobs.py`ì˜ `process_video_task`ì—ì„œë„ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©

### Alternative Approaches Considered

**Option 1: ThreadPoolExecutor ì§ì ‘ ì‚¬ìš©**
- ìž¥ì : ì œì–´í•˜ê¸° ì‰¬ì›€
- ë‹¨ì : ê¸°ì¡´ ParallelProcessor ìž¬ì‚¬ìš© ëª»í•¨, ì½”ë“œ ì¤‘ë³µ
- ì„ íƒí•˜ì§€ ì•Šì€ ì´ìœ : ê¸°ì¡´ ì¸í”„ë¼ í™œìš© ë¶ˆê°€

**Option 2: asyncioì™€ async/await ì‚¬ìš©**
- ìž¥ì : ì‹¤ì œ ë¹„ë™ê¸° ì²˜ë¦¬
- ë‹¨ì : ëª¨ë“  ì½”ë“œë¥¼ asyncë¡œ ë³€ê²½ í•„ìš”, Gemini APIê°€ sync
- ì„ íƒí•˜ì§€ ì•Šì€ ì´ìœ : ëŒ€ê·œëª¨ ë¦¬íŒ©í† ë§ í•„ìš”

**Option 3: ì„ íƒëœ ì ‘ê·¼ (ThreadPoolExecutor via ParallelProcessor)**
- ìž¥ì : ê¸°ì¡´ ì½”ë“œ ìž¬ì‚¬ìš©, ìµœì†Œ ë³€ê²½, I/O bound ìž‘ì—…ì— ì í•©
- ë‹¨ì : GILë¡œ ì¸í•œ ì„±ëŠ¥ ì œí•œ (LLM í˜¸ì¶œì€ I/O boundë¼ ì˜í–¥ ì ìŒ)
- ì„ íƒ ì´ìœ : ìµœì†Œ ë³€ê²½, ì´ë¯¸ êµ¬í˜„ëœ ì¸í”„ë¼ í™œìš©

### Benefits
- **ì„±ëŠ¥ í–¥ìƒ**: 3-5ë°° ë¹ ë¥¸ ì²˜ë¦¬ ì‹œê°„
- **ì‚¬ìš©ìž ê²½í—˜**: ë” ë¹ ë¥¸ ê²°ê³¼ ì œê³µ
- **ë¦¬ì†ŒìŠ¤ í™œìš©**: CPU ì½”ì–´ íš¨ìœ¨ì  ì‚¬ìš©
- **ê¸°ì¡´ ì½”ë“œ ìž¬ì‚¬ìš©**: ParallelProcessor í™œìš©
- **ì„¤ì • ê°€ëŠ¥**: í•„ìš” ì‹œ ìˆœì°¨ ì²˜ë¦¬ë¡œ í´ë°±

### Risks & Considerations
- **API Rate Limits**: ë³‘ë ¬ ìš”ì²­ìœ¼ë¡œ ì¸í•œ rate limit ì´ˆê³¼ ê°€ëŠ¥ì„±
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì—¬ëŸ¬ ì²­í¬ ë™ì‹œ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì¦ê°€
- **ì—ëŸ¬ ì²˜ë¦¬**: ì¼ë¶€ ì²­í¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì²˜ë¦¬ ì˜í–¥ ìµœì†Œí™” í•„ìš”
- **GIL ì˜í–¥**: Python GILì´ì§€ë§Œ LLM í˜¸ì¶œì€ I/O boundë¼ ì˜í–¥ ì ìŒ

## Testing Strategy

### Unit Tests
- `_analyze_expressions_parallel`: ë³‘ë ¬ ì²˜ë¦¬ ê²€ì¦
- `ExpressionBatchProcessor`: ì—¬ëŸ¬ ì²­í¬ ë™ì‹œ ì²˜ë¦¬ ê²€ì¦
- ì—ëŸ¬ ë°œìƒ ì‹œ ì¼ë¶€ ì²­í¬ë§Œ ì‹¤íŒ¨í•˜ëŠ”ì§€ ê²€ì¦

### Integration Tests
- ì‹¤ì œ LLM API í˜¸ì¶œì„ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬ ê²€ì¦
- ì§„í–‰ ìƒí™© ì½œë°±ì´ ì˜¬ë°”ë¥´ê²Œ ìž‘ë™í•˜ëŠ”ì§€ ê²€ì¦

### Performance Tests
- ë³‘ë ¬ vs ìˆœì°¨ ì²˜ë¦¬ ì‹œê°„ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
- ë‹¤ì–‘í•œ ì²­í¬ ìˆ˜ì— ëŒ€í•œ ì„±ëŠ¥ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •

## Files Affected

**ìˆ˜ì •:**
- `langflix/main.py` - `_analyze_expressions` ë©”ì„œë“œ ë³‘ë ¬ ì²˜ë¦¬ ì¶”ê°€
- `langflix/core/parallel_processor.py` - `ExpressionBatchProcessor`ì— `save_output` ì§€ì› ì¶”ê°€
- `langflix/settings.py` - ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • ì ‘ê·¼ìž ì¶”ê°€
- `langflix/config/default.yaml` - ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • ì¶”ê°€
- `langflix/api/routes/jobs.py` - ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© (ì„ íƒì )

**í…ŒìŠ¤íŠ¸ ì¶”ê°€:**
- `tests/unit/test_parallel_expression_analysis.py` - ë³‘ë ¬ ì²˜ë¦¬ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- `tests/integration/test_parallel_llm_integration.py` - í†µí•© í…ŒìŠ¤íŠ¸
- `tests/performance/test_parallel_vs_sequential.py` - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

## Dependencies
- Depends on: None
- Blocks: TICKET-002 (multiple expressions per context)
- Related to: None

## References
- Related code: `langflix/core/parallel_processor.py:168-229`
- Design patterns: ThreadPool Pattern, Batch Processing

## Architect Review Questions
**For the architect to consider:**
1. Gemini API rate limitì´ ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë¬¸ì œê°€ ë  ìˆ˜ ìžˆëŠ”ê°€?
2. ìµœì ì˜ worker ìˆ˜ëŠ”? (Auto-detect vs ê³ ì •ê°’)
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ê°€ ë¬¸ì œê°€ ë˜ëŠ”ê°€?
4. failover ì „ëžµì´ í•„ìš”í•œê°€?

---
## ðŸ›ï¸ Architect Review & Approval

**Reviewed by:** Architect Agent
**Review Date:** 2025-01-30
**Decision:** âœ… APPROVED (with architectural enhancements)

**Strategic Rationale:**
- Leverages existing investment - `ParallelProcessor` and `ExpressionBatchProcessor` are fully implemented but unused
- Significant performance improvement - 3-5x faster expression analysis
- Aligns with ADR-016 (Parallel LLM Processing)
- Low risk - existing infrastructure tested, can fallback to sequential
- High ROI - minimal code changes for major performance gain

**Implementation Phase:** Phase 1 - Sprint 1 (Weeks 1-2)
**Sequence Order:** #1 in implementation queue

**Architectural Guidance:**

**Critical Implementation Issue Found:**
The proposed Step 2 implementation (lines 179-195) still uses a **sequential for loop** - this defeats the purpose of parallel processing! Must use `ExpressionBatchProcessor.analyze_expression_chunks()` directly.

**Corrected Implementation Approach:**
1. **Use ExpressionBatchProcessor directly** - Don't wrap it in a sequential loop
2. **Extend ExpressionBatchProcessor** - Add `save_output` and `output_dir` parameters to `analyze_expression_chunks()`
3. **Rate Limiting Strategy:**
   - Default: `max_workers = min(cpu_count(), 5)` for Gemini API
   - Configuration: Allow override but warn if > 10
   - Circuit breaker: Track 429 errors, reduce workers if rate limited
4. **Error Handling:**
   - Partial failure tolerance - continue with successful chunks
   - Log failed chunks but don't fail entire batch
   - Retry failed chunks in sequential mode as fallback
5. **Progress Callback:**
   - Update progress as chunks complete (not sequentially)
   - Report failed chunks separately
6. **Memory Management:**
   - Consider chunking large batches (> 20 chunks)
   - Stream results if possible (don't hold all in memory)

**Dependencies:**
- **Must complete first:** None
- **Should complete first:** None
- **Blocks:** TICKET-002 (multiple expressions - parallel processing helps with larger batches)
- **Related work:** ADR-016 already documents this approach

**Risk Mitigation:**
- Risk: Gemini API rate limits (429 errors)
  - Mitigation: Conservative default (max 5 workers), exponential backoff, circuit breaker pattern
  - Monitor: Track rate limit errors, auto-adjust workers
- Risk: Memory usage with large batches
  - Mitigation: Batch chunking (> 20 chunks = process in sub-batches)
  - Monitor: Track memory usage, add limits if needed
- Risk: Partial failures affecting user experience
  - Mitigation: Graceful degradation - continue with successful chunks, log failures
  - Fallback: Retry failed chunks sequentially
- **Rollback strategy:** Simple - set `parallel_processing.enabled: false` in config to revert to sequential

**Enhanced Success Criteria:**
Beyond original ticket criteria:
- [ ] Actually uses `ExpressionBatchProcessor` (not sequential loop)
- [ ] `save_output` parameter supported in `ExpressionBatchProcessor`
- [ ] Rate limit handling with circuit breaker
- [ ] Performance benchmark: 3x+ improvement for 10+ chunks
- [ ] Memory usage acceptable (< 500MB increase for 20 chunks)
- [ ] Partial failure handling tested (some chunks fail)
- [ ] Configuration allows tuning worker count
- [ ] Integration tests verify parallel vs sequential produce same results

**Alternative Approaches Considered:**
- Original proposal: Wrapper function (but has sequential loop bug) - Rejected (doesn't actually parallelize)
- Alternative 1: Use `process_expressions_parallel` convenience function - Considered, but needs extension
- Alternative 2: Direct `ExpressionBatchProcessor` usage - âœ… Selected (cleanest, most direct)
- **Selected approach:** Extend `ExpressionBatchProcessor` to support `save_output`, use directly in pipeline

**Implementation Notes:**
- Start by: Extending `ExpressionBatchProcessor.analyze_expression_chunks()` to accept `save_output` and `output_dir`
- Watch out for: Sequential loops that defeat parallelization (CRITICAL: Step 2 implementation bug)
- Coordinate with: None
- Reference: `langflix/core/parallel_processor.py:184-229` for current implementation, `ADR-016` for design decisions

**Critical Fix Required:**
The Step 2 implementation code (lines 179-195) is **incorrect** - it still processes chunks sequentially! Must be:
```python
# CORRECT: Use ExpressionBatchProcessor directly
processor = ExpressionBatchProcessor(max_workers=max_workers)
all_results = processor.analyze_expression_chunks(
    chunks_to_process,
    language_level=language_level,
    language_code=self.language_code,
    save_output=save_llm_output,
    output_dir=output_dir,
    progress_callback=progress_callback
)

# Flatten results
all_expressions = []
for chunk_results in all_results:
    all_expressions.extend(chunk_results)
```

**Estimated Timeline:** 2-3 days (with testing and rate limit handling)
**Recommended Owner:** Senior engineer (requires understanding of concurrent processing and API limits)

## Success Criteria
How do we know this is successfully implemented?
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ 3ë°° ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
- [ ] ëª¨ë“  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë³‘ë ¬/ìˆœì°¨ ì²˜ë¦¬ ëª¨ë‘ ì§€ì›
- [ ] ì§„í–‰ ìƒí™© ì½œë°± ì •ìƒ ìž‘ë™
- [ ] ì„¤ì •ìœ¼ë¡œ í™œì„±í™”/ë¹„í™œì„±í™” ê°€ëŠ¥
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ graceful degradation
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í—ˆìš© ë²”ìœ„ ë‚´

